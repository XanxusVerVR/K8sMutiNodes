如遇到以下錯誤
TASK [remove-node/remove-etcd-node : Lookup etcd member id] *****************************************************************
fatal: [node4]: FAILED! => {"msg": "The field 'environment' has an invalid value, which includes an undefined variable. The error was: 'fallback_ips' is undefined\n\nThe error appears to be in '/home/deploy/kubespray-release-2.14/roles/remove-node/remove-etcd-node/tasks/main.yml': line 27, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n- name: Lookup etcd member id\n  ^ here\n"}
...ignoring
Wednesday 23 December 2020  18:28:37 +0800 (0:00:00.060)       0:00:16.840 ****

TASK [remove-node/remove-etcd-node : Remove etcd member from cluster] *******************************************************
fatal: [node4]: FAILED! => {"msg": "The conditional check 'etcd_member_id.stdout | length > 0' failed. The error was: error while evaluating conditional (etcd_member_id.stdout | length > 0): 'dict object' has no attribute 'stdout'\n\nThe error appears to be in '/home/deploy/kubespray-release-2.14/roles/remove-node/remove-etcd-node/tasks/main.yml': line 44, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n- name: Remove etcd member from cluster\n  ^ here\n"}
會出現這錯誤應該是第一次下移除節點的指令：
ansible-playbook -i inventory/mycluster/inventory.ini remove-node.yml -b -v -e node=node3
請回到inventory.ini，把要移除的節點，從inventory.ini的etcd區塊移除
再執行一次：
ansible-playbook -i inventory/mycluster/inventory.ini remove-node.yml -b -v -e node=node3
應該就會成功

如遇到以下錯誤
ERROR! no action detected in task. This often indicates a misspelled module name, or incorrect module path
在部屬機安裝ansible-modules-hashivault
pip install ansible-modules-hashivault

如遇到以下錯誤
to use the 'ssh' connection type with passwords, you must install the sshpass program"
在部屬機安裝sshpass
sudo apt-get install sshpass -y

如遇到以下錯誤
TASK [prep_kubeadm_images | Copy kubeadm binary from download dir to system path] ******************
fatal: [test1 -> 10.1.7.152]: FAILED! => {"changed": false, "cmd": "sshpass", "msg": "[Errno 2] No such file or directory", "rc": 2}
去master機安裝sshpass，在root執行以下指令
yum -y install sshpass

如遇到以下錯誤
fatal: [node1]: FAILED! => {"msg": "The conditional check 'kube_service_addresses | ipaddr('net')' failed. The error was: The ipaddr filter requires python's netaddr be installed on the ansible controller"}
cd ~kubespray
pip install netaddr
pip install -r requirements.txt

如遇到以下錯誤
TASK [etcd : Configure | Wait for etcd cluster to be healthy] **************************************
fatal: [test1]: FAILED! => {"attempts": 4, "changed": false, "cmd": "set -o pipefail && /usr/local/bin/etcdctl endpoint --cluster status && /usr/local/bin/etcdctl endpoint --cluster health 2>&1 | grep -q -v 'Error: unhealthy cluster'", "delta": "0:00:05.021414", "end": "2020-10-14 15:17:10.927380", "msg": "non-zero return code", "rc": 1, "start": "2020-10-14 15:17:05.905966", "stderr": "{\"level\":\"warn\",\"ts\":\"2020-10-14T15:17:10.924Z\",\"caller\":\"clientv3/retry_interceptor.go:61\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"endpoint://client-3e0d4a58-d777-47dd-802a-13e217bdc7a2/10.0.2.15:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest connection error: connection error: desc = \\\"transport: Error while dialing dial tcp 10.0.2.15:2379: connect: connection refused\\\"\"}\nError: failed to fetch endpoints from etcd cluster member list: context deadline exceeded", "stderr_lines": ["{\"level\":\"warn\",\"ts\":\"2020-10-14T15:17:10.924Z\",\"caller\":\"clientv3/retry_interceptor.go:61\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"endpoint://client-3e0d4a58-d777-47dd-802a-13e217bdc7a2/10.0.2.15:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest connection error: connection error: desc = \\\"transport: Error while dialing dial tcp 10.0.2.15:2379: connect: connection refused\\\"\"}", "Error: failed to fetch endpoints from etcd cluster member list: context deadline exceeded"], "stdout": "", "stdout_lines": []}
可試裝Chrony來校準時間
或將etcd.yml中的etcd_peer_client_auth設為false
I noticed a feature. If you run the playbook from any node of the future cluster, then everything goes without errors.
If you start from the host ansible, then only changing the variable in etcd.yml helps
etcd_peer_client_auth: false
But this is not safe, so I consider the problem relevant.

卡在以下地方
TASK [container-engine/docker : Configure docker repository on RedHat/CentOS/Oracle Linux] *****************************changed: [test1] => {"changed": true, "checksum": "d2464d85680ed702486261ec170276873ec04035", "dest": "/etc/yum.repos.d/docker-ce.repo", "gid": 0, "group": "root", "md5sum": "2758d6a9c6e40bd3b686aac3bad2dc1f", "mode": "0644", "owner": "root", "secontext": "system_u:object_r:system_conf_t:s0", "size": 192, "src": "/root/.ansible/tmp/ansible-tmp-1602664906.81-258561677559415/source", "state": "file", "uid": 0}
changed: [test3] => {"changed": true, "checksum": "d2464d85680ed702486261ec170276873ec04035", "dest": "/etc/yum.repos.d/docker-ce.repo", "gid": 0, "group": "root", "md5sum": "2758d6a9c6e40bd3b686aac3bad2dc1f", "mode": "0644", "owner": "root", "secontext": "system_u:object_r:system_conf_t:s0", "size": 192, "src": "/root/.ansible/tmp/ansible-tmp-1602664906.86-247370856306725/source", "state": "file", "uid": 0}
changed: [test2] => {"changed": true, "checksum": "d2464d85680ed702486261ec170276873ec04035", "dest": "/etc/yum.repos.d/docker-ce.repo", "gid": 0, "group": "root", "md5sum": "2758d6a9c6e40bd3b686aac3bad2dc1f", "mode": "0644", "owner": "root", "secontext": "system_u:object_r:system_conf_t:s0", "size": 192, "src": "/root/.ansible/tmp/ansible-tmp-1602664906.85-113164698282807/source", "state": "file", "uid": 0}
請將Host Guest的防火牆都關掉

可更改重試次數
kubespray/roles/etcd/tasks/configure.yml
在 Configure | Wait for etcd cluster to be healthy 這個tack的deley這行，可把random+號後的數字增加
delay: "{{ retry_stagger | random + 60 }}"

如遇到以下錯誤
TASK [kubernetes/kubeadm : Set ca.crt file permission] *********************************************************************
ok: [node1] => {"changed": false, "gid": 0, "group": "root", "mode": "0644", "owner": "root", "path": "/etc/kubernetes/ssl/ca.crt", "secontext": "unconfined_u:object_r:etc_t:s0", "size": 1066, "state": "file", "uid": 0}
ok: [node2] => {"changed": false, "gid": 0, "group": "root", "mode": "0644", "owner": "root", "path": "/etc/kubernetes/ssl/ca.crt", "secontext": "unconfined_u:object_r:etc_t:s0", "size": 1066, "state": "file", "uid": 0}
fatal: [node4]: FAILED! => {"changed": false, "msg": "file (/etc/kubernetes/ssl/ca.crt) is absent, cannot continue", "path": "/etc/kubernetes/ssl/ca.crt", "state": "absent"}

NO MORE HOSTS LEFT *********************************************************************************************************

PLAY RECAP *****************************************************************************************************************
ip=10.1.169.153            : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
ip=10.1.6.243              : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
ip=10.1.6.48               : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
localhost                  : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
node1                      : ok=468  changed=11   unreachable=0    failed=0    skipped=708  rescued=0    ignored=0
node2                      : ok=429  changed=10   unreachable=0    failed=0    skipped=480  rescued=0    ignored=0
node4                      : ok=447  changed=44   unreachable=0    failed=1    skipped=467  rescued=0    ignored=0
還沒解

部署K8s監控面板的文章
https://bit.ly/376yk6D

curl -k https://10.1.7.152:30864/

如遇到以下錯誤
fatal: [test1]: FAILED! => {"msg": "Timeout (12s) waiting for privilege escalation prompt: "}
請檢查SSH相關的連線設定 如IP Port是否正確

master機(CentOS7)
先切到root
su
密碼vagrant

yum -y install epel-release && yum -y update && yum -y install htop iftop iotop vim sshpass net-tools
(Ubuntu 18.04)
apt-get update -y && apt-get install sshpass -y

setenforce 0
sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
swapoff -a && echo "vm.swappiness=0" >> /etc/sysctl.conf && sysctl -p && free –h
systemctl disable firewalld && systemctl stop firewalld
iptables -P FORWARD ACCEPT
modprobe br_netfilter
echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables
mkdir ~/.ssh
vi ~/.ssh/authorized_keys
vi /etc/ssh/sshd_config
PermitRootLogin yes
systemctl restart sshd.service

Port 2220
部屬機的public key放在master機的root vagrant .ssh底下

部屬機可用root SSH到master機的設定
在master機不要創新的root密碼 用vagrant的即可
/etc/ssh/sshd_config
#       $OpenBSD: sshd_config,v 1.100 2016/08/15 12:32:04 naddy Exp $

# This is the sshd server system-wide configuration file.  See
# sshd_config(5) for more information.

# This sshd was compiled with PATH=/usr/local/bin:/usr/bin

# The strategy used for options in the default sshd_config shipped with
# OpenSSH is to specify options with their default value where
# possible, but leave them commented.  Uncommented options override the
# default value.

# If you want to change the port on a SELinux system, you have to tell
# SELinux about this change.
# semanage port -a -t ssh_port_t -p tcp #PORTNUMBER
#
Port 2220
#AddressFamily any
#ListenAddress 0.0.0.0
#ListenAddress ::

HostKey /etc/ssh/ssh_host_rsa_key
#HostKey /etc/ssh/ssh_host_dsa_key
HostKey /etc/ssh/ssh_host_ecdsa_key
HostKey /etc/ssh/ssh_host_ed25519_key

# Ciphers and keying
#RekeyLimit default none

# Logging
#SyslogFacility AUTH
SyslogFacility AUTHPRIV
#LogLevel INFO

# Authentication:

#LoginGraceTime 2m
#PermitRootLogin yes
#StrictModes yes
#MaxAuthTries 6
#MaxSessions 10

#PubkeyAuthentication yes

# The default is to check both .ssh/authorized_keys and .ssh/authorized_keys2
# but this is overridden so installations will only check .ssh/authorized_keys
AuthorizedKeysFile      .ssh/authorized_keys

#AuthorizedPrincipalsFile none

#AuthorizedKeysCommand none
#AuthorizedKeysCommandUser nobody

# For this to work you will also need host keys in /etc/ssh/ssh_known_hosts
#HostbasedAuthentication no
# Change to yes if you don't trust ~/.ssh/known_hosts for
# HostbasedAuthentication
#IgnoreUserKnownHosts no
# Don't read the user's ~/.rhosts and ~/.shosts files
#IgnoreRhosts yes

# To disable tunneled clear text passwords, change to no here!
#PasswordAuthentication yes
#PermitEmptyPasswords no
PasswordAuthentication no

# Change to no to disable s/key passwords
#ChallengeResponseAuthentication yes
ChallengeResponseAuthentication no

# Kerberos options
#KerberosAuthentication no
#KerberosOrLocalPasswd yes
#KerberosTicketCleanup yes
#KerberosGetAFSToken no
#KerberosUseKuserok yes

# GSSAPI options
GSSAPIAuthentication yes
GSSAPICleanupCredentials no
#GSSAPIStrictAcceptorCheck yes
#GSSAPIKeyExchange no
#GSSAPIEnablek5users no

# Set this to 'yes' to enable PAM authentication, account processing,
# and session processing. If this is enabled, PAM authentication will
# be allowed through the ChallengeResponseAuthentication and
# PasswordAuthentication.  Depending on your PAM configuration,
# PAM authentication via ChallengeResponseAuthentication may bypass
# the setting of "PermitRootLogin without-password".
# If you just want the PAM account and session checks to run without
# PAM authentication, then enable this but set PasswordAuthentication
# and ChallengeResponseAuthentication to 'no'.
# WARNING: 'UsePAM no' is not supported in Red Hat Enterprise Linux and may cause several
# problems.
UsePAM yes

#AllowAgentForwarding yes
#AllowTcpForwarding yes
#GatewayPorts no
X11Forwarding yes
#X11DisplayOffset 10
#X11UseLocalhost yes
#PermitTTY yes
#PrintMotd yes
#PrintLastLog yes
#TCPKeepAlive yes
#UseLogin no
#UsePrivilegeSeparation sandbox
#PermitUserEnvironment no
#Compression delayed
#ClientAliveInterval 0
#ClientAliveCountMax 3
#ShowPatchLevel no
#UseDNS yes
UseDNS no
#PidFile /var/run/sshd.pid
#MaxStartups 10:30:100
#PermitTunnel no
#ChrootDirectory none
#VersionAddendum none

# no default banner path
#Banner none

# Accept locale-related environment variables
AcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES
AcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT
AcceptEnv LC_IDENTIFICATION LC_ALL LANGUAGE
AcceptEnv XMODIFIERS

# override default of no subsystems
Subsystem       sftp    /usr/libexec/openssh/sftp-server

# Example of overriding settings on a per-user basis
#Match User anoncvs
#       X11Forwarding no
#       AllowTcpForwarding no
#       PermitTTY no
#       ForceCommand cvs server
只有改port

部屬機的/etc/ansible/hosts 裡面只有這樣
[targets]
10.1.7.152    ansible_connection=ssh    ansible_user=root



部屬機的/home/vagrant/kubespray/inventory/mycluster/inventory.ini 裡面是這樣
# ## Configure 'ip' variable to bind kubernetes services on a
# ## different ip than the default iface
# ## We should set etcd_member_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value.
[all]
node1 ansible_host=10.1.6.246 ansible_ssh_user=root ansible_ssh_pass=26597795
ip=10.1.6.246 etcd_member_name=etcd1

node2 ansible_host=10.1.6.51 ansible_ssh_user=root ansible_ssh_pass=26597795
ip=10.1.6.51 etcd_member_name=etcd2

node4 ansible_host=10.1.169.153 ansible_ssh_user=root ansible_ssh_pass=26597795
ip=10.1.169.153 etcd_member_name=etcd3

[kube-master]
node1
node2
node4

[etcd]
node1
node2

[kube-node]
node1
node2
node4

[calico-rr]

[k8s-cluster:children]
kube-master
kube-node


# 找到metrics_server_enabled: false 換成 metrics_server_enabled: true
sed -i 's/metrics_server_enabled: false/metrics_server_enabled: true/g' file.txt

# 找到某字串 在那個字串下面一行插入新的字串
sed -i '/\[etcd\]/a node1' inv2.ini


git clone https://github.com/kubernetes-sigs/kubespray.git -b release-2.14



重設root密碼
sudo passwd root

部屬機步驟
產生public key並放到master機
cd ~/kubespray
sudo pip3 install -r requirements.txt
cp -rfp inventory/sample inventory/mycluster
確定~/kubespray/inventory/mycluster/group_vars/k8s-cluster/addons.yml中的dashboard_enabled與metrics_server_enabled為true
pip install netaddr
declare -a IPS=(10.1.7.152 10.1.7.60 10.1.7.158)
CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}
編輯~/kubespray/inventory/mycluster/inventory.ini
ansible-playbook -i inventory/mycluster/inventory.ini --become --become-user=root cluster.yml -b -v

安裝的時間
8~18分
加節點的時間
ansible-playbook -i inventory/mycluster/$INVENTORY_FILE --become --become-user=root scale.yml -b -v
5:30內
移除節點的時間, 如果要移除的節點沒在線上，那就加reset_nodes=false
ansible-playbook -i inventory/mycluster/$INVENTORY_FILE remove-node.yml -b -v -e node=node3 -e reset_nodes=false
ansible-playbook -i inventory/mycluster/$INVENTORY_FILE remove-node.yml -b -v -e node=node5
ansible-playbook -i inventory/mycluster/$INVENTORY_FILE remove-node.yml -b -v -e node=k8s-metal-2,k8s-metal-3
不到5分鐘

還原到沒Kubernetes的狀態
ansible-playbook -i inventory/mycluster/$INVENTORY_FILE reset.yml --become --become-user=root -b -v

改kubernetes版本
kubespray/inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml
